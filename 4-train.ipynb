{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import get_linear_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "import math\n",
    "from tqdm import trange \n",
    "import gc\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "import logging\n",
    "import copy\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='torch_logs.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info(\"Testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time       ->  Window Frame Size <br>\n",
    "1 seconds  ->  32  <br>\n",
    "3 seconds  ->  94  <br>\n",
    "6 seconds  ->  188 <br>\n",
    "9 seconds  ->  282 <br>\n",
    "12 seconds ->  375 <br>\n",
    "15 seconds ->  469 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_PROCCESS = bool(True)\n",
    "PROCESS_STEP = int(100)\n",
    "\n",
    "TYPE_METRIC = str(\"f1\")\n",
    "FINE_TUNNING = bool(True)\n",
    "SAVE_BEST_MODEL = bool(True)\n",
    "# WINDOW_FRAME = int(188)#int(94) #15 seconds\n",
    "\n",
    "# MODEL TRAINING\n",
    "EPOCHS = int(100)\n",
    "BATCH_SIZE = int(200)\n",
    "lr = float(2.3e-6)\n",
    "\n",
    "model_config = {\n",
    "    \"in_channels_cnn\" : int(1),\n",
    "    \"out_channels_cnn\" : int(128),\n",
    "    \"num_heads\" : int(1),\n",
    "    \"num_layers\" : int(1),\n",
    "    \"classes\" : int(20)\n",
    "}\n",
    "\n",
    "\n",
    "# INIT SCHEDULER FOR TEST DIFFERENT LR\n",
    "\n",
    "POWER = float(4.0)\n",
    "NUM_WARMUP_STEPS= float(2) \n",
    "NUM_TRAINING_STEPS = EPOCHS\n",
    "\n",
    "LEARNING_RATE_OR_SCHEDULE = True #schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load files that contains path for songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./dataset/train.csv\")\n",
    "test_df = pd.read_csv(\"./dataset/test.csv\")\n",
    "validation_df = pd.read_csv(\"./dataset/validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1, random_state=54)\n",
    "test_df = test_df.sample(frac=1, random_state=54)\n",
    "validation_df = validation_df.sample(frac=1, random_state=54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19380</th>\n",
       "      <td>19380</td>\n",
       "      <td>radiohead</td>\n",
       "      <td>./dataset/train/radiohead-Karma_Police-3sec-50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58724</th>\n",
       "      <td>58724</td>\n",
       "      <td>cure</td>\n",
       "      <td>./dataset/train/cure-Give_Me_It-3sec-29.plk.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96554</th>\n",
       "      <td>96554</td>\n",
       "      <td>roxette</td>\n",
       "      <td>./dataset/train/roxette-What_s_She_Like_-3sec-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39551</th>\n",
       "      <td>39551</td>\n",
       "      <td>beatles</td>\n",
       "      <td>./dataset/train/beatles-I_ll_Cry_Instead-3sec-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7442</th>\n",
       "      <td>7442</td>\n",
       "      <td>prince</td>\n",
       "      <td>./dataset/train/prince-The_Max-3sec-61.plk.gz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0      label  \\\n",
       "19380       19380  radiohead   \n",
       "58724       58724       cure   \n",
       "96554       96554    roxette   \n",
       "39551       39551    beatles   \n",
       "7442         7442     prince   \n",
       "\n",
       "                                                    path  \n",
       "19380  ./dataset/train/radiohead-Karma_Police-3sec-50...  \n",
       "58724     ./dataset/train/cure-Give_Me_It-3sec-29.plk.gz  \n",
       "96554  ./dataset/train/roxette-What_s_She_Like_-3sec-...  \n",
       "39551  ./dataset/train/beatles-I_ll_Cry_Instead-3sec-...  \n",
       "7442       ./dataset/train/prince-The_Max-3sec-61.plk.gz  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data for loading in DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spectrogram_path = train_df.path.values \n",
    "train_spectrogram_labels = train_df.label.values \n",
    "\n",
    "test_spectrogram_path = test_df.path.values \n",
    "test_spectrogram_labels = test_df.label.values \n",
    "\n",
    "validation_spectrogram_path = validation_df.path.values \n",
    "validation_spectrogram_labels = validation_df.label.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['aerosmith', 'beatles', 'creedence_clearwater_revival', 'cure',\n",
       "        'dave_matthews_band', 'depeche_mode', 'fleetwood_mac',\n",
       "        'garth_brooks', 'green_day', 'led_zeppelin', 'madonna',\n",
       "        'metallica', 'prince', 'queen', 'radiohead', 'roxette',\n",
       "        'steely_dan', 'suzanne_vega', 'tori_amos', 'u2'], dtype=object)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse_output = False).fit(train_spectrogram_labels.reshape(-1, 1))\n",
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_encoded = encoder.transform(train_spectrogram_labels.reshape(-1, 1))\n",
    "test_labels_encoded = encoder.transform(test_spectrogram_labels.reshape(-1, 1))\n",
    "validation_labels_encoded = encoder.transform(validation_spectrogram_labels.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Class for opening file and create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongDatasetLoader(Dataset):\n",
    "    \"\"\"Load songs spectrograms from path\"\"\"\n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray, transform: bool = True) -> None:\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.DTYPE = torch.float\n",
    "   \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "        data_path_idx = self.data[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        with gzip.open(data_path_idx, 'rb') as f:\n",
    "            # Load the decompressed data\n",
    "            decompressed_data = pickle.load(f)\n",
    "            decompressed_data = decompressed_data['spectrogram']\n",
    "\n",
    "    \n",
    "        if self.transform:\n",
    "            # Convert to tensor without specifying dtype\n",
    "            decompressed_data = torch.tensor(decompressed_data, dtype=self.DTYPE )\n",
    "            labels = torch.tensor(labels, dtype = self.DTYPE )\n",
    "           \n",
    "\n",
    "        return decompressed_data, labels\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_load_data = SongDatasetLoader(train_spectrogram_path, train_labels_encoded)\n",
    "test_load_data = SongDatasetLoader(test_spectrogram_path, test_labels_encoded)\n",
    "Validation_load_data = SongDatasetLoader(validation_spectrogram_path, validation_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataLoader = DataLoader(train_load_data, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_dataLoader = DataLoader(test_load_data, batch_size = BATCH_SIZE, shuffle = False)\n",
    "validation_dataLoader = DataLoader(Validation_load_data, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels:int, out_channels:int)-> Tensor:\n",
    "        super(CNN, self).__init__()\n",
    "        padding = [(1,1), (0,0)]\n",
    "        number_filters = [int(64), int(64),  out_channels]\n",
    "\n",
    "        self.cnn = nn.Sequential(OrderedDict([\n",
    "            (\"batch_norm\",nn.BatchNorm2d(1)),\n",
    "            (\"conv_1\", nn.Conv2d(in_channels = in_channels, out_channels=number_filters[0], kernel_size=(3,3), padding=padding[0])),\n",
    "            (\"act_1\", nn.ELU()),\n",
    "            (\"max_pooling_1\",nn.MaxPool2d(kernel_size=(4,4), stride=(4,2), padding = padding[0])),\n",
    "            (\"dropout_1\",nn.Dropout(p = 0.2)),\n",
    "\n",
    "            (\"conv_2\", nn.Conv2d(in_channels = number_filters[0], out_channels=number_filters[1], kernel_size=(4,4), padding=padding[0])),\n",
    "            (\"act_2\", nn.ELU()),\n",
    "            (\"max_pooling_2\",nn.MaxPool2d(kernel_size=(4,2), stride=(4,1), padding = padding[0])),\n",
    "            (\"dropout_2\",nn.Dropout(p = 0.2)),\n",
    "\n",
    "            (\"conv_3\", nn.Conv2d(in_channels = number_filters[1], out_channels=number_filters[2], kernel_size=(4,4), padding=padding[0])),\n",
    "            (\"act_3\", nn.ELU()),\n",
    "            (\"max_pooling_3\",nn.MaxPool2d(kernel_size=(4,2), stride=(4,1), padding = padding[1])),\n",
    "            (\"dropout_3\",nn.Dropout(p = 0.2)),\n",
    "\n",
    "        ]))\n",
    "\n",
    "\n",
    "    def forward(self, src:Tensor) -> Tensor:\n",
    "        \n",
    "        src = src.unsqueeze(1)\n",
    "        src =  self.cnn(src)\n",
    "       \n",
    "        src = src.squeeze(2)\n",
    "        # src = src.contiguous().view(src.size(0), 256, 50 * 8)  #torch.Size([batch = 128, 512, 6 * 46])   \n",
    "        return src \n",
    "        \n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.encoding[:, :x.size(1), :].to(device)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class TransformerEncoderMusic(nn.Module):\n",
    "    def __init__(self, max_seq_len: int, \n",
    "                 embedding_dim: int, \n",
    "                 num_heads: int,  \n",
    "                 num_layers: int, \n",
    "                 dropout: float = 0.2,\n",
    "                 output: int=20):\n",
    "        \n",
    "        super(TransformerEncoderMusic, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Linear(max_seq_len, embedding_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim) \n",
    "        self.norm = nn.LayerNorm(embedding_dim, eps = 1e-6)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(embedding_dim, \n",
    "                                                    nhead = num_heads,  \n",
    "                                                    dropout=dropout, \n",
    "                                                    dim_feedforward = embedding_dim,\n",
    "                                                    # activation = \"gelu\",\n",
    "                                                    layer_norm_eps = 1e-12)\n",
    "        \n",
    "        self.transformer_encoder  = nn.TransformerEncoder(encoder_layers, \n",
    "                                                          num_layers =  num_layers, \n",
    "                                                          norm = self.norm\n",
    "                                                          )\n",
    "\n",
    "        self.output = nn.Sequential(OrderedDict([\n",
    "            # (\"dense\",  nn.Linear(in_features= embedding_dim, out_features=embedding_dim)),\n",
    "            # (\"activation\", nn.Tanh()),\n",
    "            # (\"activation\", nn.GELU()),\n",
    "            # (\"dropout\", nn.Dropout(p=0.2)),\n",
    "            (\"classifier\", nn.Linear(in_features=embedding_dim, out_features=output)),\n",
    "            (\"dropout\", nn.Dropout(p=0.2)),\n",
    "        ]))\n",
    "\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        src = self.embedding(src) * math.sqrt(self.embedding_dim)\n",
    "        # src = print(src.size())\n",
    "        # src = src.permute(2,0,1)\n",
    "      \n",
    "        src = self.norm(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        src = src.permute(2,0,1)\n",
    "        src = self.transformer_encoder(src)\n",
    "        # src, _ = src.max(dim=0)\n",
    "        src = src.mean(dim=0)\n",
    " \n",
    "        src = self.output(src)\n",
    "   \n",
    "        return src\n",
    "    \n",
    "\n",
    "\n",
    "class VisualEncoderModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels_cnn: int, \n",
    "                 out_channels_cnn:int, \n",
    "                 classes:int = 20, \n",
    "                 num_heads : int=2,\n",
    "                 num_layers : int = 1,\n",
    "                 )-> Tensor:\n",
    "        \n",
    "        super(VisualEncoderModel, self).__init__()\n",
    "        self.cnn = CNN(in_channels_cnn, out_channels_cnn)\n",
    "        \n",
    "        self.encoder = TransformerEncoderMusic(max_seq_len = 45, \n",
    "                                          embedding_dim = out_channels_cnn, \n",
    "                                          num_heads = num_heads,  \n",
    "                                          num_layers = num_layers, \n",
    "                                          output=classes)\n",
    "                \n",
    "    def forward(self, src: torch.Tensor):\n",
    "        src = self.cnn(src) \n",
    "        src = self.encoder(src)\n",
    "        return src\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VisualEncoderModel(**model_config)\n",
    "\n",
    "# model = nn.DataParallel(model)\n",
    "# model.to(device)\n",
    "\n",
    "# for epoch in trange(2, desc=\"Epoch\"):\n",
    "  \n",
    "#   model.train()\n",
    " \n",
    "#   for step, batch in enumerate(train_dataLoader):\n",
    "#     batch = tuple(t.to(device) for t in batch)\n",
    "#     input_ids, labels = batch\n",
    "    \n",
    "    \n",
    "#     output = model(input_ids)\n",
    "#     print(output.size())\n",
    "#     loss = criterion(output, labels)\n",
    "\n",
    "    \n",
    "#     # print(output)\n",
    "\n",
    "#     break\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INIT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = VisualEncoderModel(**model_config)\n",
    "\n",
    "# model = nn.DataParallel(model)\n",
    "# model.to(device)\n",
    "# logging.info(model.parameters)\n",
    "\n",
    "# model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of DataParallel(\n",
       "  (module): DataParallel(\n",
       "    (module): VisualEncoderModel(\n",
       "      (cnn): CNN(\n",
       "        (cnn): Sequential(\n",
       "          (batch_norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv_1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (act_1): ELU(alpha=1.0)\n",
       "          (max_pooling_1): MaxPool2d(kernel_size=(4, 4), stride=(4, 2), padding=(1, 1), dilation=1, ceil_mode=False)\n",
       "          (dropout_1): Dropout(p=0.2, inplace=False)\n",
       "          (conv_2): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "          (act_2): ELU(alpha=1.0)\n",
       "          (max_pooling_2): MaxPool2d(kernel_size=(4, 2), stride=(4, 1), padding=(1, 1), dilation=1, ceil_mode=False)\n",
       "          (dropout_2): Dropout(p=0.2, inplace=False)\n",
       "          (conv_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "          (act_3): ELU(alpha=1.0)\n",
       "          (max_pooling_3): MaxPool2d(kernel_size=(4, 2), stride=(4, 1), padding=(0, 0), dilation=1, ceil_mode=False)\n",
       "          (dropout_3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder): TransformerEncoderMusic(\n",
       "        (embedding): Linear(in_features=45, out_features=128, bias=True)\n",
       "        (pos_encoder): PositionalEncoding()\n",
       "        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (transformer_encoder): TransformerEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TransformerEncoderLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.2, inplace=False)\n",
       "              (dropout2): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (output): Sequential(\n",
       "          (classifier): Linear(in_features=128, out_features=20, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_txt = \"./model/47_epoch_2.2956857088918255e-06_lr_1716644414_timestap.pt\"\n",
    "model = torch.load(model_txt)\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "logging.info(f\"Loading model {model_txt}\")\n",
    "logging.info( model.parameters)\n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3e-06"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = AdamW(model.parameters(), lr = lr, weight_decay=0.023 )\n",
    "optimizer = Adam(model.parameters(), lr = lr, weight_decay=0.02 )\n",
    "\n",
    "total_steps = len(train_load_data) * EPOCHS\n",
    "\n",
    "# scheduler = get_polynomial_decay_schedule_with_warmup(optimizer,\n",
    "#                                                       num_warmup_steps=0,\n",
    "#                                                       num_training_steps=total_steps,\n",
    "#                                                       power=POWER)\n",
    "\n",
    "\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "\n",
    "# scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "#                                             num_warmup_steps=NUM_WARMUP_STEPS,\n",
    "#                                             num_training_steps=NUM_TRAINING_STEPS)\n",
    "\n",
    "# Define the step size and gamma for the scheduler\n",
    "# step_size = 20\n",
    "# gamma = 0.1\n",
    "\n",
    "# # Define the scheduler\n",
    "# scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(labels : Tensor , output : Tensor, typeMetric: str = 'f1' ) -> float:\n",
    "  from sklearn.metrics import f1_score\n",
    "\n",
    "  y_pred = output.detach().cpu().numpy()\n",
    "  y_true = labels.detach().cpu().numpy()\n",
    "\n",
    "  pred_flat = np.argmax(y_pred, axis=1).flatten()\n",
    "  labels_flat = np.argmax(y_true, axis=1).flatten()\n",
    "\n",
    "  # print(pred_flat)\n",
    "  # print(labels_flat)\n",
    "  # print(\"---------------------\")\n",
    " \n",
    "  if typeMetric == 'f1':\n",
    "    return f1_score(labels_flat, pred_flat, average='micro')\n",
    "  if typeMetric == 'acc':\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing() -> None:\n",
    "    test_loss = float(0)\n",
    "    metric_evaluation = float(0)\n",
    "    accuracy_evaluation = float(0)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad(): #  not to compute or store gradients\n",
    "      for _, batch in enumerate(test_dataLoader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, labels = batch #unpack from Dataloader\n",
    "        output = model(input_ids)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        metric_evaluation += metrics(labels, output, typeMetric = TYPE_METRIC)\n",
    "        accuracy_evaluation  += metrics(labels, output, typeMetric = 'acc')\n",
    "        \n",
    "    txt = f'Test Evaluation {TYPE_METRIC}: {(metric_evaluation/len(test_dataLoader)):3.5f} |' \\\n",
    "    f'acc: {(accuracy_evaluation/len(test_dataLoader)):3.5f} | loss: {(test_loss/len(test_dataLoader)):3.5} |'\\\n",
    "    f'Execution time: {(time.time() - start_time):5.2f}'\n",
    "    logging.info(txt)\n",
    "    print(txt)\n",
    "      \n",
    "    # clean memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Process: epochs 0/99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mab/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stage: | epoch   0/99 |   100/  580 batches |lr 0.000002300 | ms/batch 144.64 | loss 2.608300686 | accuracy 0.2373 | f1 0.2373\n",
      "Training stage: | epoch   0/99 |   200/  580 batches |lr 0.000002300 | ms/batch 279.06 | loss 2.538571358 | accuracy 0.2377 | f1 0.2377\n",
      "Training stage: | epoch   0/99 |   300/  580 batches |lr 0.000002300 | ms/batch 428.63 | loss 2.622751713 | accuracy 0.2388 | f1 0.2388\n",
      "Training stage: | epoch   0/99 |   400/  580 batches |lr 0.000002300 | ms/batch 592.96 | loss 2.535528898 | accuracy 0.2376 | f1 0.2376\n",
      "Training stage: | epoch   0/99 |   500/  580 batches |lr 0.000002300 | ms/batch 765.29 | loss 2.621937752 | accuracy 0.2390 | f1 0.2390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mab/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.5938690473293438 | accuracy 0.23817684320823013 | f1 0.23817684320823013 |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Validation Process: epochs 0/99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mab/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "Epoch:   1%|          | 1/100 [01:34<2:35:32, 94.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Evaluation f1: 0.30728 |acc: 0.30728 | loss: 2.44341149 |Execution time:  4.38\n",
      "======================================================================================================================================================\n",
      "Training Process: epochs 1/99\n",
      "Training stage: | epoch   1/99 |   100/  580 batches |lr 0.000002300 | ms/batch 160.69 | loss 2.591262102 | accuracy 0.2432 | f1 0.2432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   1%|          | 1/100 [02:04<3:24:50, 124.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Train the data for one epoch\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataLoader):\n\u001b[1;32m     28\u001b[0m   batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(t\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch)\n\u001b[1;32m     29\u001b[0m   input_ids, labels \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36mSongDatasetLoader.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gzip\u001b[38;5;241m.\u001b[39mopen(data_path_idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Load the decompressed data\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     decompressed_data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     19\u001b[0m     decompressed_data \u001b[38;5;241m=\u001b[39m decompressed_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspectrogram\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Convert to tensor without specifying dtype\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/gzip.py:301\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01merrno\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mEBADF, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread() on write-only GzipFile object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer\u001b[38;5;241m.\u001b[39mread(size)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m(byte_view))\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/gzip.py:505\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_member \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# Read a chunk of data from the file\u001b[39;00m\n\u001b[0;32m--> 505\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE)\n\u001b[1;32m    507\u001b[0m uncompress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39mdecompress(buf, size)\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/gzip.py:97\u001b[0m, in \u001b[0;36m_PaddedFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m     94\u001b[0m read \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[read:] \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m---> 97\u001b[0m        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile\u001b[38;5;241m.\u001b[39mread(size\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_length\u001b[38;5;241m+\u001b[39mread)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "best_epoch = int(0)\n",
    "# best_by_metric = float('-inf')\n",
    "best_by_metric = float(0.3125273417734863)\n",
    "best_model = copy.deepcopy(model.state_dict())\n",
    "file_name = str(\"\")\n",
    "check_if_model_saved = bool(False)\n",
    "\n",
    "train_loss_set = []\n",
    "\n",
    "for epoch in trange(EPOCHS, desc=\"Epoch\"):\n",
    "  print(f'Training Process: epochs {epoch}/{EPOCHS - 1}')\n",
    "  # Training --------------------------------------------------------------------------------------------------------\n",
    "\n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "\n",
    "\n",
    "  \n",
    "  training_loss = float(0)\n",
    "  training_metric_evaluation = float(0)\n",
    "  training_accuracy_evaluation = float(0)\n",
    "  log_interval = PROCESS_STEP\n",
    "  start_time = time.time()\n",
    "\n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataLoader):\n",
    "    \n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, labels = batch\n",
    "    output = model(input_ids)\n",
    "\n",
    "    loss = criterion(output, labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "\n",
    "    training_loss += loss.item()\n",
    "\n",
    "    training_metric_evaluation += metrics(labels, output, typeMetric = TYPE_METRIC)\n",
    "    training_accuracy_evaluation  += metrics(labels, output, typeMetric = 'acc')\n",
    "    train_loss_set.append(loss.item())\n",
    "\n",
    "    \n",
    "    if SHOW_PROCCESS and (step % log_interval == 0 and step > 0):\n",
    "        lr_show = scheduler.get_last_lr()[0] if LEARNING_RATE_OR_SCHEDULE else lr\n",
    "        ms_per_batch = (time.time() - start_time) * 1000 / log_interval # time in miliseconds\n",
    "        txt = f'Training stage: | epoch {epoch:3d}/{EPOCHS - 1} | {step:5d}/{len(train_dataLoader):5d} batches |'\\\n",
    "              f'lr {lr_show:02.9f} | ms/batch {ms_per_batch:5.2f} | loss {loss.item():5.9f} |'\\\n",
    "              f' accuracy {(training_accuracy_evaluation / step):2.4f} | {TYPE_METRIC} {(training_metric_evaluation/step):2.4f}' \n",
    "\n",
    "        logging.info(txt)\n",
    "        print(txt) \n",
    "        \n",
    "    if LEARNING_RATE_OR_SCHEDULE:\n",
    "      scheduler.step()\n",
    "\n",
    "  txt = f\"Train loss {training_loss/len(train_dataLoader)} | accuracy {training_accuracy_evaluation/len(train_dataLoader)} | {TYPE_METRIC} {training_metric_evaluation/len(train_dataLoader)} |\"\n",
    "  logging.info(txt)\n",
    "  print(txt)\n",
    "    \n",
    "  # clean memory\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  \n",
    "  print('-' * 150)\n",
    "  logging.info('-' * 150)\n",
    "  # Validation -----------------------------------------------------------------------------------------------------------\n",
    "  print(f'Validation Process: epochs {epoch}/{EPOCHS - 1}')\n",
    "  validation_loss = float(0)\n",
    "  metric_evaluation = float(0)\n",
    "  accuracy_evaluation = float(0)\n",
    "\n",
    "\n",
    "  start_time = time.time()\n",
    "  log_interval = PROCESS_STEP\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  model.eval()\n",
    "  with torch.no_grad(): #  not to compute or store gradients\n",
    "    for step, batch in enumerate(validation_dataLoader):\n",
    "    \n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      input_ids, labels = batch #unpack from Dataloader\n",
    "    \n",
    "      output = model(input_ids)\n",
    "      loss = criterion(output, labels)\n",
    "\n",
    "      validation_loss += loss.item()\n",
    "      metric_evaluation += metrics(labels, output, typeMetric = TYPE_METRIC)\n",
    "      accuracy_evaluation  += metrics(labels, output, typeMetric = 'acc')\n",
    "      \n",
    "      \n",
    "\n",
    "      if SHOW_PROCCESS and (step % log_interval == 0 and step > 0):\n",
    "        lr_show = scheduler.get_last_lr()[0] if LEARNING_RATE_OR_SCHEDULE else lr\n",
    "        ms_per_batch = (time.time() - start_time) * 1000 / log_interval # time in miliseconds\n",
    "        txt = f'Validation stage: | epoch {epoch:3d}/{EPOCHS - 1} | {step:5d}/{len(validation_dataLoader):5d} batches |'\\\n",
    "              f'lr {lr_show:02.9f} | ms/batch {ms_per_batch:5.2f} | loss {loss.item():5.9f} |'\\\n",
    "              f'Validation Evaluation {TYPE_METRIC}: {(metric_evaluation/step):3.5f} |'\\\n",
    "              f'acc: {(accuracy_evaluation/step):3.5f} |'\n",
    "        \n",
    "        logging.info(txt)\n",
    "        print(txt)  \n",
    "\n",
    "    \n",
    "    if SAVE_BEST_MODEL and ((metric_evaluation/len(validation_dataLoader)) > best_by_metric):\n",
    "      best_by_metric = metric_evaluation/len(validation_dataLoader)\n",
    "      best_Model =  copy.deepcopy(model.state_dict())\n",
    "      best_epoch = epoch\n",
    "\n",
    "      lr_show = scheduler.get_last_lr()[0] if LEARNING_RATE_OR_SCHEDULE else lr\n",
    "      file_name = f'./model/{epoch}_epoch_{lr_show}_lr_{int(time.time())}_timestap.pt'\n",
    "      torch.save(model, file_name)\n",
    "      logging.info(f'File saved {file_name}')\n",
    "      print(f'File saved {file_name}') \n",
    "      check_if_model_saved = True\n",
    "\n",
    "\n",
    "    txt = f'Validation Evaluation {TYPE_METRIC}: {(metric_evaluation/len(validation_dataLoader)):3.5f} |' \\\n",
    "    f'acc: {(accuracy_evaluation/len(validation_dataLoader)):3.5f} | loss: {(validation_loss/len(validation_dataLoader)):3.9} |'\\\n",
    "    f'Execution time: {(time.time() - start_time):5.2f}'  \n",
    "\n",
    "    logging.info(txt)\n",
    "    print(txt)\n",
    "\n",
    "  \n",
    "\n",
    "  # clean memory\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "  if check_if_model_saved:\n",
    "    txt = f'Best model saved: | {best_epoch} epoch | metric {TYPE_METRIC}: {best_by_metric} | model file saved: {file_name}'\n",
    "    logging.info(txt)\n",
    "    print(txt)\n",
    "    check_if_model_saved = False\n",
    "  \n",
    "  \n",
    "  print('=' * 150)\n",
    "  logging.info('=' * 150)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of DataParallel(\n",
       "  (module): VisualEncoderModel(\n",
       "    (cnn): CNN(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ELU(alpha=1.0)\n",
       "        (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ELU(alpha=1.0)\n",
       "        (8): MaxPool2d(kernel_size=(4, 1), stride=(4, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "        (9): Dropout(p=0.2, inplace=False)\n",
       "        (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (12): ELU(alpha=1.0)\n",
       "        (13): MaxPool2d(kernel_size=(3, 1), stride=(3, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "        (14): Dropout(p=0.2, inplace=False)\n",
       "        (15): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (16): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (17): ELU(alpha=1.0)\n",
       "        (18): MaxPool2d(kernel_size=(3, 1), stride=(3, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "        (19): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (encoder): TransformerEncoderMusic(\n",
       "      (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (pos_encoder): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "            (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.3, inplace=False)\n",
       "            (dropout2): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pooler): Pooler(\n",
       "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (classifier): Sequential(\n",
       "        (0): Dropout(p=0.2, inplace=False)\n",
       "        (1): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): Dropout(p=0.2, inplace=False)\n",
       "        (4): Linear(in_features=64, out_features=20, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(\"./model/9_epoch_0.00018352070914127424_lr_1716471410_timestap.pt\").to(device)\n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"cnn\":model.cnn.cnn.state_dict()}, \"./model_pretrained/cnn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation f1: 0.39979 |acc: 0.42514 | loss: 1.9445 |Execution time: 36.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mab/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super(CNN, self).__init__()\n",
    "        filters = [128, 128, 128, out_channels]\n",
    "        kernel_size = (3,3)\n",
    "        pool_size = [(2, 2), (4, 1), (3 , 1)] \n",
    "        self.cnn = nn.Sequential(\n",
    "            # nn.BatchNorm2d(in_channels),\n",
    "            nn.Conv2d(in_channels, out_channels=filters[0], kernel_size=kernel_size, padding=1),\n",
    "            nn.ELU(),\n",
    "            # nn.BatchNorm2d(filters[0]), nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size[0], stride=pool_size[0]),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(filters[0], out_channels=filters[1], kernel_size=kernel_size, padding=1),\n",
    "            nn.ELU(),\n",
    "            # nn.BatchNorm2d(filters[1]), nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size[1], stride=pool_size[1]),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(filters[1], out_channels=filters[2], kernel_size=kernel_size, padding=1),\n",
    "            nn.ELU(),\n",
    "            # nn.BatchNorm2d(filters[2]), nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size[2], stride=pool_size[2]),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(filters[2], out_channels=filters[3], kernel_size=kernel_size, padding=1),\n",
    "            nn.ELU(),\n",
    "            # nn.BatchNorm2d(filters[3]), nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size[2], stride=pool_size[2]),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # self.fc1 = nn.Linear(128 * 1 * 47, 512)  # Adjust the input size based on the input dimensions\n",
    "        # self.dropout = nn.Dropout(0.5)\n",
    "        # self.fc2 = nn.Linear(512, 20)\n",
    "\n",
    "    def forward(self, src:Tensor) -> Tensor:\n",
    "    \n",
    "        src = src.unsqueeze(1)\n",
    "        src =  self.cnn(src)\n",
    "       \n",
    "        # src = src.view(-1, 128 * 1 * 47)\n",
    "        # x = F.relu(self.fc1(src))\n",
    "        # x = self.dropout(x)\n",
    "        # x = self.fc2(x)\n",
    "        src = src.squeeze(2)       \n",
    "        return src\n",
    "\n",
    "\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, embedding_dim, eps:float= 1e-12):\n",
    "        super(Norm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embedding_dim,eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.norm(x)\n",
    "\n",
    "    \n",
    "# # The positional encoding vector, embedding_dim is d_model\n",
    "# class PositionalEncoder(nn.Module):\n",
    "#     def __init__(self, embedding_dim, max_seq_length=512, dropout=0.1):\n",
    "#         super(PositionalEncoder, self).__init__()\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         pe = torch.zeros(max_seq_length, embedding_dim)\n",
    "#         for pos in range(max_seq_length):\n",
    "#             for i in range(0, embedding_dim, 2):\n",
    "#                 pe[pos, i] = math.sin(pos/(10000**(2*i/embedding_dim)))\n",
    "#                 pe[pos, i+1] = math.cos(pos/(10000**((2*i+1)/embedding_dim)))\n",
    "#         pe = pe.unsqueeze(0)        \n",
    "#         self.register_buffer('pe', pe)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = x*math.sqrt(self.embedding_dim)\n",
    "#         seq_length = x.size(1)\n",
    "#         pe = Variable(self.pe[:, :seq_length], requires_grad=False).to(x.device)\n",
    "#         # Add the positional encoding vector to the embedding vector\n",
    "#         x = x + pe\n",
    "#         x = self.dropout(x)\n",
    "#         return x\n",
    "    \n",
    "        \n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        # self.dense = nn.Linear(d_model,  d_model)\n",
    "        self.dense = nn.Linear(d_model,  20)\n",
    "        # self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, src):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        # first_token_tensor = hidden_states[:, 0]\n",
    "        # src, _ = src.max(dim=1)\n",
    "        src = src.mean(dim=0)\n",
    "        pooled_output = self.dense(src)\n",
    "        # pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "    \n",
    "\n",
    "# Transformer encoder layer\n",
    "# class EncoderLayer(nn.Module):\n",
    "#     def __init__(self, embedding_dim, num_heads, ff_dim=2048, dropout=0.1):\n",
    "#         super(EncoderLayer, self).__init__()\n",
    "#         self.self_attention = nn.MultiheadAttention(embedding_dim, num_heads, dropout)\n",
    "#         self.feed_forward = nn.Sequential(\n",
    "#             nn.Linear(embedding_dim, ff_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(ff_dim, embedding_dim)\n",
    "#         )\n",
    "#         self.dropout1 = nn.Dropout(dropout)\n",
    "#         self.dropout2 = nn.Dropout(dropout)\n",
    "#         self.norm1 = Norm(embedding_dim)\n",
    "#         self.norm2 = Norm(embedding_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x2 = self.norm1(x)\n",
    "#         # Add and Muti-head attention\n",
    "#         # x = x + self.dropout1(self.self_attention(x2, x2, x2, mask))\n",
    "#         x = x + self.dropout1(self.self_attention(x2, x2, x2))\n",
    "#         x2 = self.norm2(x)\n",
    "#         x = x + self.dropout2(self.feed_forward(x2))\n",
    "#         return x\n",
    "\n",
    "class TransformerEncoderMusic(nn.Module):\n",
    "    def __init__(self, max_seq_len: int, \n",
    "                 embedding_dim: int, \n",
    "                 num_heads: int,  \n",
    "                 num_layers: int, \n",
    "                 dropout: float = 0.5):\n",
    "        \n",
    "        super(TransformerEncoderMusic, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.embedding_dim = embedding_dim\n",
    "      \n",
    "        # self.layernorm = nn.LayerNorm(d_model, eps=1e-5)\n",
    "         \n",
    "        # self.pos_encoder = PositionalEncoder(embedding_dim, dropout= 0.3, max_seq_length = max_seq_len)  # Max time length\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # self.layers = nn.ModuleList([EncoderLayer(embedding_dim, num_heads, 2048, dropout) for _ in range(num_layers)])\n",
    "        # self.layers = nn.ModuleList([EncoderLayer(embedding_dim, num_heads, 1024, dropout) for _ in range(num_layers)])\n",
    "        self.norm = Norm(embedding_dim)\n",
    "        # self.position_embedding = PositionalEncoder(embedding_dim, max_seq_len, dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(embedding_dim, \n",
    "                                                    nhead = num_heads,  \n",
    "                                                    dropout=dropout, \n",
    "                                                    # activation = 'gelu', \n",
    "                                                    dim_feedforward = embedding_dim,\n",
    "                                                    batch_first = False)\n",
    "        \n",
    "        self.transformer_encoder  = nn.TransformerEncoder(encoder_layers, \n",
    "                                                          num_layers =  num_layers, \n",
    "                                                          norm = self.norm, \n",
    "                                                          enable_nested_tensor=True)\n",
    "        self.Intermediate = nn.Sequential(\n",
    "            nn.Linear(embedding_dim,1024),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(1024, embedding_dim),\n",
    "            nn.ELU(), # borrar\n",
    "            nn.LayerNorm(embedding_dim, eps=1e-12),\n",
    "            nn.Dropout(p = 0.1)\n",
    "        )\n",
    "\n",
    "        self.pooler = Pooler(embedding_dim)\n",
    "        \n",
    "        # self.classifier= nn.Sequential(\n",
    "        #     nn.Linear(embedding_dim, 20),\n",
    "        #     # nn.Dropout(0.2),\n",
    "        #     # nn.Linear(128, 64),\n",
    "        #     # nn.ReLU(),\n",
    "        #     # nn.Dropout(0.2),\n",
    "        #     # nn.Linear(64, 20),\n",
    "        #     # nn.Dropout(0.2),\n",
    "        # )\n",
    "        \n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        \n",
    "        src = src.permute(0, 2, 1)\n",
    "        # src = self.pos_encoder(src)\n",
    " \n",
    "        # src = src * math.sqrt(self.embedding_dim) \n",
    "        src = self.transformer_encoder(src)\n",
    "        src = self.Intermediate(src)\n",
    "        src = self.output(src)\n",
    "        # src, _ = src.max(dim=1)\n",
    "        # sequence_output = encoder_outputs[0]\n",
    "        src = self.pooler(src)\n",
    "        # src = self.classifier(src)\n",
    "        \n",
    "            \n",
    "        return src\n",
    " \n",
    "\n",
    "class VisualEncoderModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels_cnn: int, \n",
    "                 out_channels_cnn:int, \n",
    "                 classes:int = 20, \n",
    "                 n_head : int=2,\n",
    "                 n_layers : int = 1,\n",
    "                 dropout_transformer : float = 0.2,\n",
    "                 dropout_classifier: float = 0.2)-> Tensor:\n",
    "        \n",
    "        super(VisualEncoderModel, self).__init__()\n",
    "        self.cnn = CNN(in_channels_cnn, out_channels_cnn)\n",
    "        \n",
    "        self.encoder = TransformerEncoderMusic(max_seq_len = 78, \n",
    "                                          embedding_dim = out_channels_cnn, \n",
    "                                          num_heads = n_head,  \n",
    "                                          num_layers = n_layers, \n",
    "                                          dropout = dropout_transformer)\n",
    "    \n",
    "    def forward(self, src: torch.Tensor):\n",
    "       \n",
    "        src = self.cnn(src) \n",
    "        \n",
    "        src = self.encoder(src)\n",
    "        return src\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super(CNN, self).__init__()\n",
    "        filters = [64, 128, 128, out_channels]\n",
    "        kernel_size = (3,3)\n",
    "        pool_size = [(2, 2), (4, 1), (3 , 1)] \n",
    "        self.cnn = nn.Sequential(\n",
    "            # nn.BatchNorm2d(in_channels),\n",
    "            nn.Conv2d(in_channels, out_channels=filters[0], kernel_size=kernel_size, padding=1),\n",
    "            nn.BatchNorm2d(filters[0]), nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size[0], stride=pool_size[0]),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(filters[0], out_channels=filters[1], kernel_size=kernel_size, padding=1),\n",
    "            nn.BatchNorm2d(filters[1]), nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size[1], stride=pool_size[1]),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(filters[1], out_channels=filters[2], kernel_size=kernel_size, padding=1),\n",
    "            nn.BatchNorm2d(filters[2]), nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size[2], stride=pool_size[2]),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(filters[2], out_channels=filters[3], kernel_size=kernel_size, padding=1),\n",
    "            nn.BatchNorm2d(filters[3]), nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size[2], stride=pool_size[2]),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    " \n",
    "\n",
    "    def forward(self, src:Tensor) -> Tensor:\n",
    "    \n",
    "        src = src.unsqueeze(1)\n",
    "        src =  self.cnn(src)\n",
    "\n",
    "        src = src.squeeze(2)       \n",
    "        return src\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: `embeddings`, shape (batch, max_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "            `encoder input`, shape (batch, max_len, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, : x.size(1)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "        \n",
    "         \n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, ntoken: int, \n",
    "                 d_model: int, \n",
    "                 n_head: int,  \n",
    "                 nlayers: int, \n",
    "                 dropout: float = 0.5):\n",
    "        \n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "         \n",
    "        self.pos_encoder = PositionalEncoding(d_model = d_model, dropout= 0.3, max_len=ntoken)  # Max time length\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, \n",
    "                                                    nhead = n_head,  \n",
    "                                                    dropout=dropout, \n",
    "                                               \n",
    "                                                    dim_feedforward = 1024,\n",
    "                                                    batch_first = False)\n",
    "        \n",
    "        self.transformer_encoder  = nn.TransformerEncoder(encoder_layers, num_layers =  nlayers)\n",
    "\n",
    "        self.classifier= nn.Sequential(\n",
    "     \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 20),\n",
    "\n",
    "        )\n",
    "        \n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        \n",
    "        src = src.permute(0, 2, 1)\n",
    "        \n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        src = self.transformer_encoder(src)\n",
    "        src, _ = src.max(dim=1)\n",
    "        \n",
    "        src = self.classifier(src)\n",
    "        \n",
    "            \n",
    "        return src\n",
    "    \n",
    "\n",
    "class VisualEncoderModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels_cnn: int, \n",
    "                 out_channels_cnn:int, \n",
    "                 classes:int = 20, \n",
    "                 n_head : int=2,\n",
    "                 n_layers : int = 1,\n",
    "                 dropout_transformer : float = 0.2,\n",
    "                 dropout_classifier: float = 0.2)-> Tensor:\n",
    "        \n",
    "        super(VisualEncoderModel, self).__init__()\n",
    "        self.cnn = CNN(in_channels_cnn, out_channels_cnn)\n",
    "        \n",
    "        self.encoder = TransformerEncoder(ntoken = 157, \n",
    "                                          d_model = out_channels_cnn, \n",
    "                                          n_head = n_head,  \n",
    "                                          nlayers = n_layers, \n",
    "                                          dropout = dropout_transformer)\n",
    "        # self.classifier= nn.Sequential(\n",
    "        #     nn.Linear(out_channels_cnn, 128),\n",
    "        #     nn.ELU(),\n",
    "        #     nn.Dropout(dropout_classifier),\n",
    "        #     nn.Linear(128, 64),\n",
    "        #     nn.ELU(),\n",
    "        #     nn.Dropout(dropout_classifier),\n",
    "        #     nn.Linear(64, classes),\n",
    "        #     nn.Dropout(dropout_classifier),\n",
    "        # )\n",
    "\n",
    "        # self.classifier= nn.Sequential(\n",
    "        #     nn.Linear(47, classes),\n",
    "        #     nn.Dropout(dropout_classifier)\n",
    "        # )\n",
    "        # self.initialize_weights()\n",
    "\n",
    "    # def initialize_weights(self):\n",
    "    #     for m in self.classifier.modules():\n",
    "    #         if isinstance(m, nn.Linear):\n",
    "    #             nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "    #             if m.bias is not None:\n",
    "    #                 nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, src: torch.Tensor):\n",
    "       \n",
    "        src = self.cnn(src) \n",
    "        \n",
    "        src = self.encoder(src)\n",
    "        # src = self.classifier(src)\n",
    "        \n",
    "\n",
    "        return src\n",
    "        # batch, filters, _, _ = src.size()\n",
    "        # src = src.permute(0, 2, 3, 1)  # New shape: [48, 8, 47, 128]\n",
    "        # src = src.contiguous().view(batch, -1, filters)  # New shape: [48, 8*47, 128]\n",
    "      \n",
    "        # src = src.squeeze(2) \n",
    "        # src = self.encoder(src)\n",
    "        # print(src.size())\n",
    "        # return self.classifier(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class CNNTransformerModel(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(CNNTransformerModel, self).__init__()\n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Positional encoding for transformers\n",
    "        self.positional_encoding = PositionalEncoding(128)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8, dim_feedforward=512)\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers=3)\n",
    "        \n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(128 * 47, num_classes)  # 128 channels, reduced size of 47 after pooling\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 1, 128, 94)\n",
    "        \n",
    "        # Apply CNN layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # shape: (batch_size, 32, 64, 47)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # shape: (batch_size, 64, 32, 23)\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # shape: (batch_size, 128, 16, 11)\n",
    "        \n",
    "        # Flatten for transformer input\n",
    "        x = x.view(x.size(0), 128, -1).permute(2, 0, 1)  # shape: (11*16, batch_size, 128)\n",
    "        \n",
    "        # Apply positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Apply transformer layers\n",
    "        x = self.transformer(x)  # shape: (11*16, batch_size, 128)\n",
    "        \n",
    "        # Flatten and fully connected layer\n",
    "        x = x.permute(1, 0, 2).contiguous().view(x.size(1), -1)  # shape: (batch_size, 128 * 47)\n",
    "        x = self.fc(x)  # shape: (batch_size, num_classes)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Example usage\n",
    "model = CNNTransformerModel(num_classes=20)\n",
    "input_tensor = torch.randn(8, 1, 128, 94)  # batch_size=8\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Expected output shape: (8, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super(CNN, self).__init__()\n",
    "        filters = [128, 128, 128, out_channels]\n",
    "        kernel_size = (3,3)\n",
    "        pool_size = [(2, 2), (4, 1), (3 , 1)] \n",
    "        self.cnn = nn.Sequential(\n",
    "            # nn.BatchNorm2d(in_channels),\n",
    "            nn.Conv2d(in_channels, out_channels=filters[0], kernel_size=kernel_size, padding=1),\n",
    "            nn.ELU(),\n",
    "            # nn.BatchNorm2d(filters[0]), nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size[0], stride=pool_size[0]),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(filters[0], out_channels=filters[1], kernel_size=kernel_size, padding=1),\n",
    "            nn.ELU(),\n",
    "            # nn.BatchNorm2d(filters[1]), nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size[1], stride=pool_size[1]),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(filters[1], out_channels=filters[2], kernel_size=kernel_size, padding=1),\n",
    "            nn.ELU(),\n",
    "            # nn.BatchNorm2d(filters[2]), nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size[2], stride=pool_size[2]),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(filters[2], out_channels=filters[3], kernel_size=kernel_size, padding=1),\n",
    "            nn.ELU(),\n",
    "            # nn.BatchNorm2d(filters[3]), nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size[2], stride=pool_size[2]),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # self.fc1 = nn.Linear(128 * 1 * 47, 512)  # Adjust the input size based on the input dimensions\n",
    "        # self.dropout = nn.Dropout(0.5)\n",
    "        # self.fc2 = nn.Linear(512, 20)\n",
    "\n",
    "    def forward(self, src:Tensor) -> Tensor:\n",
    "    \n",
    "        src = src.unsqueeze(1)\n",
    "        src =  self.cnn(src)\n",
    "       \n",
    "        # src = src.view(-1, 128 * 1 * 47)\n",
    "        # x = F.relu(self.fc1(src))\n",
    "        # x = self.dropout(x)\n",
    "        # x = self.fc2(x)\n",
    "        src = src.squeeze(2)       \n",
    "        return src\n",
    "\n",
    "\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, embedding_dim, eps:float= 1e-12):\n",
    "        super(Norm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embedding_dim,eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.norm(x)\n",
    "\n",
    "    \n",
    "# The positional encoding vector, embedding_dim is d_model\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_seq_length=512, dropout=0.1):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_seq_length, embedding_dim)\n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(0, embedding_dim, 2):\n",
    "                pe[pos, i] = math.sin(pos/(10000**(2*i/embedding_dim)))\n",
    "                pe[pos, i+1] = math.cos(pos/(10000**((2*i+1)/embedding_dim)))\n",
    "        pe = pe.unsqueeze(0)        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x*math.sqrt(self.embedding_dim)\n",
    "        seq_length = x.size(1)\n",
    "        pe = Variable(self.pe[:, :seq_length], requires_grad=False).to(x.device)\n",
    "        # Add the positional encoding vector to the embedding vector\n",
    "        x = x + pe\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "        \n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        # self.dense = nn.Linear(d_model,  d_model)\n",
    "        self.dense = nn.Linear(d_model,  20)\n",
    "        # self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, src):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        # first_token_tensor = hidden_states[:, 0]\n",
    "        src, _ = src.max(dim=1)\n",
    "        # src = src.mean(dim=1)\n",
    "        pooled_output = self.dense(src)\n",
    "        # pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "    \n",
    "\n",
    "# Transformer encoder layer\n",
    "# class EncoderLayer(nn.Module):\n",
    "#     def __init__(self, embedding_dim, num_heads, ff_dim=2048, dropout=0.1):\n",
    "#         super(EncoderLayer, self).__init__()\n",
    "#         self.self_attention = nn.MultiheadAttention(embedding_dim, num_heads, dropout)\n",
    "#         self.feed_forward = nn.Sequential(\n",
    "#             nn.Linear(embedding_dim, ff_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(ff_dim, embedding_dim)\n",
    "#         )\n",
    "#         self.dropout1 = nn.Dropout(dropout)\n",
    "#         self.dropout2 = nn.Dropout(dropout)\n",
    "#         self.norm1 = Norm(embedding_dim)\n",
    "#         self.norm2 = Norm(embedding_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x2 = self.norm1(x)\n",
    "#         # Add and Muti-head attention\n",
    "#         # x = x + self.dropout1(self.self_attention(x2, x2, x2, mask))\n",
    "#         x = x + self.dropout1(self.self_attention(x2, x2, x2))\n",
    "#         x2 = self.norm2(x)\n",
    "#         x = x + self.dropout2(self.feed_forward(x2))\n",
    "#         return x\n",
    "\n",
    "class TransformerEncoderMusic(nn.Module):\n",
    "    def __init__(self, max_seq_len: int, \n",
    "                 embedding_dim: int, \n",
    "                 num_heads: int,  \n",
    "                 num_layers: int, \n",
    "                 dropout: float = 0.5):\n",
    "        \n",
    "        super(TransformerEncoderMusic, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.embedding_dim = embedding_dim\n",
    "      \n",
    "        # self.layernorm = nn.LayerNorm(d_model, eps=1e-5)\n",
    "         \n",
    "        self.pos_encoder = PositionalEncoder(embedding_dim, dropout= 0.3, max_seq_length = max_seq_len)  # Max time length\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # self.layers = nn.ModuleList([EncoderLayer(embedding_dim, num_heads, 2048, dropout) for _ in range(num_layers)])\n",
    "        # self.layers = nn.ModuleList([EncoderLayer(embedding_dim, num_heads, 1024, dropout) for _ in range(num_layers)])\n",
    "        self.norm = Norm(embedding_dim)\n",
    "        # self.position_embedding = PositionalEncoder(embedding_dim, max_seq_len, dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(embedding_dim, \n",
    "                                                    nhead = num_heads,  \n",
    "                                                    dropout=dropout, \n",
    "                                                    # activation = 'gelu', \n",
    "                                                    dim_feedforward = embedding_dim,\n",
    "                                                    batch_first = False)\n",
    "        \n",
    "        self.transformer_encoder  = nn.TransformerEncoder(encoder_layers, \n",
    "                                                          num_layers =  num_layers, \n",
    "                                                          norm = self.norm, \n",
    "                                                          enable_nested_tensor=True)\n",
    "        self.Intermediate = nn.Sequential(\n",
    "            nn.Linear(embedding_dim,1024),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(1024, embedding_dim),\n",
    "            nn.ELU(), # borrar\n",
    "            nn.LayerNorm(embedding_dim, eps=1e-12),\n",
    "            nn.Dropout(p = 0.1)\n",
    "        )\n",
    "\n",
    "        self.pooler = Pooler(embedding_dim)\n",
    "        \n",
    "        # self.classifier= nn.Sequential(\n",
    "        #     nn.Linear(embedding_dim, 20),\n",
    "        #     # nn.Dropout(0.2),\n",
    "        #     # nn.Linear(128, 64),\n",
    "        #     # nn.ReLU(),\n",
    "        #     # nn.Dropout(0.2),\n",
    "        #     # nn.Linear(64, 20),\n",
    "        #     # nn.Dropout(0.2),\n",
    "        # )\n",
    "        \n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        \n",
    "        src = src.permute(0, 2, 1)\n",
    "        src = self.pos_encoder(src)\n",
    " \n",
    "        src = src * math.sqrt(self.embedding_dim) \n",
    "        src = self.transformer_encoder(src)\n",
    "        src = self.Intermediate(src)\n",
    "        src = self.output(src)\n",
    "        # src, _ = src.max(dim=1)\n",
    "        # sequence_output = encoder_outputs[0]\n",
    "        src = self.pooler(src)\n",
    "        # src = self.classifier(src)\n",
    "        \n",
    "            \n",
    "        return src\n",
    " \n",
    "\n",
    "class VisualEncoderModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels_cnn: int, \n",
    "                 out_channels_cnn:int, \n",
    "                 classes:int = 20, \n",
    "                 n_head : int=2,\n",
    "                 n_layers : int = 1,\n",
    "                 dropout_transformer : float = 0.2,\n",
    "                 dropout_classifier: float = 0.2)-> Tensor:\n",
    "        \n",
    "        super(VisualEncoderModel, self).__init__()\n",
    "        self.cnn = CNN(in_channels_cnn, out_channels_cnn)\n",
    "        \n",
    "        self.encoder = TransformerEncoderMusic(max_seq_len = 78, \n",
    "                                          embedding_dim = out_channels_cnn, \n",
    "                                          num_heads = n_head,  \n",
    "                                          num_layers = n_layers, \n",
    "                                          dropout = dropout_transformer)\n",
    "    \n",
    "    def forward(self, src: torch.Tensor):\n",
    "       \n",
    "        src = self.cnn(src) \n",
    "        \n",
    "        src = self.encoder(src)\n",
    "        return src\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ORIGINAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super(CNN, self).__init__()\n",
    "        filters = [128, 128, 128, out_channels]\n",
    "        kernel_size = (3,3)\n",
    "        pool_size = [(2, 2), (4, 1), (3 , 1)] \n",
    "        self.cnn = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(in_channels, out_channels=filters[0], kernel_size=kernel_size, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size[0], stride=pool_size[0]),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(filters[0], out_channels=filters[1], kernel_size=kernel_size, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size[1], stride=pool_size[1]),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(filters[1], out_channels=filters[2], kernel_size=kernel_size, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size[2], stride=pool_size[2]),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(filters[2], out_channels=filters[3], kernel_size=kernel_size, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size[2], stride=pool_size[2]),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def forward(self, src:Tensor) -> Tensor:\n",
    "    \n",
    "        src = src.unsqueeze(1)\n",
    "        src =  self.cnn(src)\n",
    "        src = src.squeeze(2)       \n",
    "        return src\n",
    "    \n",
    "    def _initialize_weights(self) -> None:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    \n",
    "# The positional encoding vector, embedding_dim is d_model\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_seq_length=512, dropout=0.1):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_seq_length, embedding_dim)\n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(0, embedding_dim, 2):\n",
    "                pe[pos, i] = math.sin(pos/(10000**(2*i/embedding_dim)))\n",
    "                pe[pos, i+1] = math.cos(pos/(10000**((2*i+1)/embedding_dim)))\n",
    "        pe = pe.unsqueeze(0)        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x*math.sqrt(self.embedding_dim)\n",
    "        seq_length = x.size(1)\n",
    "        pe = Variable(self.pe[:, :seq_length], requires_grad=False).to(x.device)\n",
    "        # Add the positional encoding vector to the embedding vector\n",
    "        x = x + pe\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "class TransformerEncoderMusic(nn.Module):\n",
    "    def __init__(self, max_seq_len: int, \n",
    "                 embedding_dim: int, \n",
    "                 num_heads: int,  \n",
    "                 num_layers: int, \n",
    "                 dropout: float = 0.1,\n",
    "                 output: int=20):\n",
    "        \n",
    "        super(TransformerEncoderMusic, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.pos_encoder = PositionalEncoder(embedding_dim, max_seq_length = max_seq_len) \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.norm = nn.LayerNorm(embedding_dim,eps = 1e-12)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(embedding_dim, \n",
    "                                                    nhead = num_heads,  \n",
    "                                                    dropout=dropout, \n",
    "                                                    dim_feedforward = embedding_dim,\n",
    "                                                    # activation = \"gelu\",\n",
    "                                                    batch_first = False)\n",
    "        \n",
    "        self.transformer_encoder  = nn.TransformerEncoder(encoder_layers, \n",
    "                                                          num_layers =  num_layers, \n",
    "                                                          norm = self.norm, \n",
    "                                                          enable_nested_tensor=True)\n",
    "        self.Intermediate = nn.Sequential(\n",
    "            nn.Linear(embedding_dim,1024),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1024, embedding_dim),\n",
    "            nn.LayerNorm(embedding_dim, eps=1e-12),\n",
    "            nn.GELU(), # borrar\n",
    "            nn.Dropout(p = 0.1)\n",
    "        )\n",
    "\n",
    "        self.output = nn.Linear(embedding_dim,  output)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        \n",
    "        src = src.permute(0, 2, 1)\n",
    "        src = self.pos_encoder(src)\n",
    " \n",
    "        src = src * math.sqrt(self.embedding_dim) \n",
    "        src = self.transformer_encoder(src)\n",
    "        src = self.Intermediate(src)\n",
    "        src, _ = src.max(dim=1)\n",
    "        # src = src.mean(dim=1)\n",
    "        src = self.output(src)\n",
    "\n",
    "        return src\n",
    "    \n",
    "    def _initialize_weights(self) -> None:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    " \n",
    "\n",
    "class VisualEncoderModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels_cnn: int, \n",
    "                 out_channels_cnn:int, \n",
    "                 classes:int = 20, \n",
    "                 num_heads : int=2,\n",
    "                 num_layers : int = 1,\n",
    "                 )-> Tensor:\n",
    "        \n",
    "        super(VisualEncoderModel, self).__init__()\n",
    "        self.cnn = CNN(in_channels_cnn, out_channels_cnn)\n",
    "        \n",
    "        self.encoder = TransformerEncoderMusic(max_seq_len = 78, \n",
    "                                          embedding_dim = out_channels_cnn, \n",
    "                                          num_heads = num_heads,  \n",
    "                                          num_layers = num_layers, \n",
    "                                          output=classes)\n",
    "                \n",
    "    def forward(self, src: torch.Tensor):\n",
    "        src = self.cnn(src) \n",
    "        \n",
    "        src = self.encoder(src)\n",
    "        return src\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels:int, out_channels:int)-> Tensor:\n",
    "        super(CNN, self).__init__()\n",
    "        kernels = [(4,4),(4,2),(4,2)]\n",
    "        strides = [(2,2), (2,1), (2,1)]\n",
    "        padding = [(1,1)]\n",
    "        number_filters = [int(32), int(64), int(128),  out_channels]\n",
    "\n",
    "        self.cnn = nn.Sequential(OrderedDict([\n",
    "            (\"batch_norm\",nn.BatchNorm2d(1)),\n",
    "\n",
    "            (\"conv_1\", nn.Conv2d(in_channels = in_channels, out_channels=number_filters[0], kernel_size=(3,3), padding=padding[0])),\n",
    "            (\"act_1\", nn.ELU()),\n",
    "            (\"batch_norm_1\",nn.BatchNorm2d(number_filters[0])),\n",
    "            (\"max_pooling_1\",nn.MaxPool2d(kernel_size=kernels[0], stride=strides[0], padding = padding[0])),\n",
    "            (\"dropout_1\",nn.Dropout(p = 0.2)),\n",
    "\n",
    "            (\"conv_2\", nn.Conv2d(in_channels = number_filters[0], out_channels=number_filters[1], kernel_size=(3,3), padding=padding[0])),\n",
    "            (\"act_2\", nn.ELU()),\n",
    "            (\"batch_norm_2\",nn.BatchNorm2d(number_filters[1])),\n",
    "            (\"max_pooling_2\",nn.MaxPool2d(kernel_size=kernels[1], stride=strides[1], padding = padding[0])),\n",
    "            (\"dropout_2\",nn.Dropout(p = 0.2)),\n",
    "\n",
    "            (\"conv_3\", nn.Conv2d(in_channels = number_filters[1], out_channels=number_filters[2], kernel_size=(3,3), padding=padding[0])),\n",
    "            (\"act_3\", nn.ELU()),\n",
    "            (\"batch_norm_3\",nn.BatchNorm2d(number_filters[2])),\n",
    "            (\"max_pooling_3\",nn.MaxPool2d(kernel_size=kernels[1], stride=strides[1], padding = padding[0])),\n",
    "            (\"dropout_3\",nn.Dropout(p = 0.2)),\n",
    "\n",
    "            (\"conv_4\", nn.Conv2d(in_channels = number_filters[2], out_channels=number_filters[3], kernel_size=(3,3), padding=padding[0])),\n",
    "            (\"act_4\", nn.ELU()),\n",
    "            (\"batch_norm_4\",nn.BatchNorm2d(number_filters[3])),\n",
    "            (\"max_pooling_4\",nn.MaxPool2d(kernel_size=kernels[2], stride=strides[2], padding = padding[0])),\n",
    "            (\"dropout_4\",nn.Dropout(p = 0.2))\n",
    "        ]))\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self) -> None:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def forward(self, src:Tensor) -> Tensor:\n",
    "        \n",
    "        src = src.unsqueeze(1)\n",
    "        src =  self.cnn(src)\n",
    "        \n",
    "        # src = src.squeeze(2)\n",
    "        src = src.contiguous().view(src.size(0), 256, 50 * 8)  #torch.Size([batch = 128, 512, 6 * 46])   \n",
    "        return src #[batch, 512, 231]\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "# # The positional encoding vector, embedding_dim is d_model\n",
    "# class PositionalEncoder(nn.Module):\n",
    "#     def __init__(self, embedding_dim, max_seq_length=512, dropout=0.1):\n",
    "#         super(PositionalEncoder, self).__init__()\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         pe = torch.zeros(max_seq_length, embedding_dim)\n",
    "#         for pos in range(max_seq_length):\n",
    "#             for i in range(0, embedding_dim, 2):\n",
    "#                 pe[pos, i] = math.sin(pos/(10000**(2*i/embedding_dim)))\n",
    "#                 pe[pos, i+1] = math.cos(pos/(10000**((2*i+1)/embedding_dim)))\n",
    "#         pe = pe.unsqueeze(0)        \n",
    "#         self.register_buffer('pe', pe)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         seq_length = x.size(1)\n",
    "#         pe = Variable(self.pe[:, :seq_length], requires_grad=False).to(x.device)\n",
    "#         # Add the positional encoding vector to the embedding vector\n",
    "#         x = x + pe\n",
    "#         # x = x * math.sqrt(self.embedding_dim) + pe\n",
    "#         x = self.dropout(x)\n",
    "#         return x\n",
    "    \n",
    "class PositionalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim: int, max_seq_length: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p = 0.1)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        position = torch.arange(max_seq_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim))\n",
    "        pe = torch.zeros(max_seq_length, 1, embedding_dim)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        #[batch_size, seq_len, embedding_dim]\n",
    "        # print(self.pe[:x.size(0)].size())\n",
    "        return self.dropout(x * math.sqrt(self.embedding_dim) + self.pe[:x.size(0)] )   \n",
    "    \n",
    "\n",
    "class TransformerEncoderMusic(nn.Module):\n",
    "    def __init__(self, max_seq_len: int, \n",
    "                 embedding_dim: int, \n",
    "                 num_heads: int,  \n",
    "                 num_layers: int, \n",
    "                 dropout: float = 0.1,\n",
    "                 output: int=20):\n",
    "        \n",
    "        super(TransformerEncoderMusic, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoder(embedding_dim, max_seq_length = max_seq_len) \n",
    "        self.norm = nn.LayerNorm(embedding_dim, eps = 1e-5)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(embedding_dim, \n",
    "                                                    nhead = num_heads,  \n",
    "                                                    dropout=dropout, \n",
    "                                                    dim_feedforward = embedding_dim,\n",
    "                                                    # activation = \"gelu\",\n",
    "                                                    batch_first = False)\n",
    "        \n",
    "        self.transformer_encoder  = nn.TransformerEncoder(encoder_layers, \n",
    "                                                          num_layers =  num_layers, \n",
    "                                                          norm = self.norm, \n",
    "                                                          enable_nested_tensor=True)\n",
    "\n",
    "        self.output = nn.Sequential(OrderedDict([\n",
    "            (\"dense\",  nn.Linear(in_features= embedding_dim, out_features=128)),\n",
    "            (\"activation\", nn.Tanh()),\n",
    "            # (\"activation\", nn.GELU()),\n",
    "            (\"dropout\", nn.Dropout(p=0.2)),\n",
    "            (\"classifier\", nn.Linear(in_features=128, out_features=output)),\n",
    "        ]))\n",
    "        \n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "       \n",
    "        src = src.permute(2,0,1)\n",
    "       \n",
    "        src = self.norm(src)\n",
    "        # src = self.pos_encoder(src)\n",
    "        src = self.transformer_encoder(src)\n",
    "        src, _ = src.max(dim=0)\n",
    "        # src = src.mean(dim=0)\n",
    "        src = self.output(src)\n",
    "        return src\n",
    "    \n",
    "    def _initialize_weights(self) -> None:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    " \n",
    "\n",
    "class VisualEncoderModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels_cnn: int, \n",
    "                 out_channels_cnn:int, \n",
    "                 classes:int = 20, \n",
    "                 num_heads : int=2,\n",
    "                 num_layers : int = 1,\n",
    "                 )-> Tensor:\n",
    "        \n",
    "        super(VisualEncoderModel, self).__init__()\n",
    "        self.cnn = CNN(in_channels_cnn, out_channels_cnn)\n",
    "        \n",
    "        self.encoder = TransformerEncoderMusic(max_seq_len = 400, \n",
    "                                          embedding_dim = out_channels_cnn, \n",
    "                                          num_heads = num_heads,  \n",
    "                                          num_layers = num_layers, \n",
    "                                          output=classes)\n",
    "                \n",
    "    def forward(self, src: torch.Tensor):\n",
    "        src = self.cnn(src) \n",
    "        src = self.encoder(src)\n",
    "        return src\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels:int, out_channels:int)-> Tensor:\n",
    "        super(CNN, self).__init__()\n",
    "        padding = [(1,1), (0,0)]\n",
    "        number_filters = [int(64), int(128),  out_channels]\n",
    "\n",
    "        self.cnn = nn.Sequential(OrderedDict([\n",
    "            (\"batch_norm\",nn.BatchNorm2d(1)),\n",
    "            (\"conv_1\", nn.Conv2d(in_channels = in_channels, out_channels=number_filters[0], kernel_size=(3,3), padding=padding[0])),\n",
    "            (\"act_1\", nn.ELU()),\n",
    "            (\"max_pooling_1\",nn.MaxPool2d(kernel_size=(4,2), stride=(4,1), padding = padding[0])),\n",
    "            (\"dropout_1\",nn.Dropout(p = 0.1)),\n",
    "\n",
    "            (\"conv_2\", nn.Conv2d(in_channels = number_filters[0], out_channels=number_filters[1], kernel_size=(4,4), padding=padding[0])),\n",
    "            (\"act_2\", nn.ELU()),\n",
    "            (\"max_pooling_2\",nn.MaxPool2d(kernel_size=(4,2), stride=(4,1), padding = padding[0])),\n",
    "            (\"dropout_2\",nn.Dropout(p = 0.1)),\n",
    "\n",
    "            (\"conv_3\", nn.Conv2d(in_channels = number_filters[1], out_channels=number_filters[2], kernel_size=(4,4), padding=padding[0])),\n",
    "            (\"act_3\", nn.ELU()),\n",
    "            (\"max_pooling_3\",nn.MaxPool2d(kernel_size=(4,2), stride=(4,1), padding = padding[1])),\n",
    "            (\"dropout_3\",nn.Dropout(p = 0.1)),\n",
    "\n",
    "        ]))\n",
    "\n",
    "\n",
    "    def forward(self, src:Tensor) -> Tensor:\n",
    "        \n",
    "        src = src.unsqueeze(1)\n",
    "        src =  self.cnn(src)\n",
    "        src = src.squeeze(2)\n",
    "        # src = src.contiguous().view(src.size(0), 256, 50 * 8)  #torch.Size([batch = 128, 512, 6 * 46])   \n",
    "        return src \n",
    "        \n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.encoding[:, :x.size(1), :].to(device)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class TransformerEncoderMusic(nn.Module):\n",
    "    def __init__(self, max_seq_len: int, \n",
    "                 embedding_dim: int, \n",
    "                 num_heads: int,  \n",
    "                 num_layers: int, \n",
    "                 dropout: float = 0.1,\n",
    "                 output: int=20):\n",
    "        \n",
    "        super(TransformerEncoderMusic, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Linear(max_seq_len, embedding_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim) \n",
    "        self.norm = nn.LayerNorm(embedding_dim, eps = 1e-6)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(embedding_dim, \n",
    "                                                    nhead = num_heads,  \n",
    "                                                    dropout=dropout, \n",
    "                                                    dim_feedforward = embedding_dim,\n",
    "                                                    # activation = \"gelu\",\n",
    "                                                    batch_first = False)\n",
    "        \n",
    "        self.transformer_encoder  = nn.TransformerEncoder(encoder_layers, \n",
    "                                                          num_layers =  num_layers, \n",
    "                                                          norm = self.norm\n",
    "                                                          )\n",
    "\n",
    "        self.output = nn.Sequential(OrderedDict([\n",
    "            (\"dense\",  nn.Linear(in_features= embedding_dim, out_features=embedding_dim)),\n",
    "            (\"activation\", nn.Tanh()),\n",
    "            # (\"activation\", nn.GELU()),\n",
    "            (\"dropout\", nn.Dropout(p=0.2)),\n",
    "            (\"classifier\", nn.Linear(in_features=embedding_dim, out_features=output)),\n",
    "        ]))\n",
    "\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        src = self.embedding(src) * math.sqrt(self.embedding_dim)\n",
    "        # src = print(src.size())\n",
    "        # src = src.permute(2,0,1)\n",
    "      \n",
    "        src = self.norm(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        src = src.permute(2,0,1)\n",
    "        src = self.transformer_encoder(src)\n",
    "        # src, _ = src.max(dim=0)\n",
    "        src = src.mean(dim=0)\n",
    " \n",
    "        src = self.output(src)\n",
    "   \n",
    "        return src\n",
    "    \n",
    "\n",
    "\n",
    "class VisualEncoderModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels_cnn: int, \n",
    "                 out_channels_cnn:int, \n",
    "                 classes:int = 20, \n",
    "                 num_heads : int=2,\n",
    "                 num_layers : int = 1,\n",
    "                 )-> Tensor:\n",
    "        \n",
    "        super(VisualEncoderModel, self).__init__()\n",
    "        self.cnn = CNN(in_channels_cnn, out_channels_cnn)\n",
    "        \n",
    "        self.encoder = TransformerEncoderMusic(max_seq_len = 93, \n",
    "                                          embedding_dim = out_channels_cnn, \n",
    "                                          num_heads = num_heads,  \n",
    "                                          num_layers = num_layers, \n",
    "                                          output=classes)\n",
    "                \n",
    "    def forward(self, src: torch.Tensor):\n",
    "        src = self.cnn(src) \n",
    "        src = self.encoder(src)\n",
    "        return src\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
